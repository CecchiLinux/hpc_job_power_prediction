{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import errno    \n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Author: Enrico Ceccolini\n",
    "    TODO write the description\n",
    "'''\n",
    "\n",
    "datadir = \"/datasets/eurora_data/db1/\"\n",
    "datadir = \"/datasets/eurora_data/db/\"\n",
    "infile_jobs_to_nodes = datadir + \"job_nodes.csv\"\n",
    "\n",
    "\n",
    "infile_nodes = datadir + \"nodes.csv\"\n",
    "\n",
    "suffix = \"_5sec_\"\n",
    "### select an interval from\n",
    "## 1 settings wholeData\n",
    "interval_comment_whole = \"WholeData\"\n",
    "\n",
    "### select an interval from\n",
    "## 2 settings Andrea\n",
    "interval_comment = \"Andrea\"\n",
    "start_time = pd.to_datetime('2014-03-31')\n",
    "end_time = pd.to_datetime('2014-05-01')\n",
    "infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "## 3 settings Alina\n",
    "#interval_comment = \"Alina\"\n",
    "#start_time = pd.to_datetime('2014-06-30')\n",
    "#end_time = pd.to_datetime('2014-11-01')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the nodes we have three differentIntel Xeon E5 processors stepping: nodes 1-16 & 25-32 havea maximum frequency of 2.1GHz (CPUs-2100), nodes 17-24 have a maximum frequency of 2.2GHz (CPUs-2200), andnodes 33-64 have a maximum frequency of 3.1GHz (CPUs-3100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.read_csv(infile_nodes, index_col=0)\n",
    "# nodes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read job2nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data contains 469095 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = pd.read_csv(infile_jobs_to_nodes, index_col=0)\n",
    "print(\"jobs_to_nodes_whole_data contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of jobs runned on node 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data now contains 469020 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data[jobs_to_nodes_whole_data['node_id'] != 129]\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of duplicates job_id_string, node_id\n",
    "this is probably caused by the concurrency writing of the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data now contains 444610 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data.drop_duplicates(subset=['job_id_string', 'node_id'])\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read jobs data\n",
    "\n",
    "drop the jobs out of interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_whole_data contains 88537 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "jobs_whole = pd.read_csv(infile_jobs + \".csv\", index_col=0)\n",
    "print(\"jobs_whole_data contains {} records\".format(jobs_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94962\n"
     ]
    }
   ],
   "source": [
    "merged_jobs_to_nodes_long = pd.merge(jobs_to_nodes_whole_data, jobs_whole, on='job_id_string')\n",
    "print(merged_jobs_to_nodes_long.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop the unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I will probably create a new dataframe \"job_id_string: real_power\"\n",
    "# add two new column: real_power and entire_node\n",
    "# completed_jobs_data_interval['entire_node'] = pd.Series()\n",
    "# completed_jobs_data_interval['real_power'] = pd.Series(0)\n",
    "# completed_jobs_data_interval.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the real power consumption - long jobs (4h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "jobs_whole['real_pow'] = 0.0\n",
    "jobs_whole['runned_alone'] = True\n",
    "jobs_whole['real_pow_quality'] = 0.0\n",
    "\n",
    "jobs_whole['n_2_1'] = 0\n",
    "jobs_whole['n_2_2'] = 0\n",
    "jobs_whole['n_3_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(real_pow.shape[0])\n",
    "#print(runned_alone.shape[0])\n",
    "#print(quality.shape[0])\n",
    "\n",
    "#large_jobs_tail = large_jobs[large_jobs.index > 390345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs_tail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow.csv\"\n",
    "for index, row in jobs_whole.iterrows():\n",
    "#for index, row in large_jobs_tail.iterrows():\n",
    "    print()\n",
    "    print(\"{}/{}\".format(i, jobs_whole.shape[0]))\n",
    "\n",
    "    #real_pow[i], runned_alone[i], quality[i] = calculate_job_consumption(row['job_id_string'])\n",
    "    real_pow_val, runned_alone_val, quality_val, job_n_2_1, job_n_2_2, job_n_3_1 = calculate_job_consumption_2(row['job_id_string'])\n",
    "    jobs_whole.at[index,'real_pow'] = real_pow_val\n",
    "    jobs_whole.at[index,'runned_alone'] = runned_alone_val\n",
    "    jobs_whole.at[index,'real_pow_quality'] = quality_val\n",
    "    \n",
    "    jobs_whole.at[index,'n_2_1'] = job_n_2_1\n",
    "    jobs_whole.at[index,'n_2_2'] = job_n_2_2\n",
    "    jobs_whole.at[index,'n_3_1'] = job_n_3_1\n",
    "    \n",
    "    i = i + 1\n",
    "    if(i % 1000 == 0):\n",
    "        jobs_whole.to_csv(outfile)\n",
    "        \n",
    "    #    break\n",
    "\n",
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "jobs_whole.to_csv(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "#outfile = datadir + 'CPUs/' + interval_comment + \"/large_jobs_real_pow_3.csv\"\n",
    "#large_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"01\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_01 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"02\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_02 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"03\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_03 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"04\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_04 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"05\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_05 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"06\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_06 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"07\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_07 = pd.read_csv(infile_node)\n",
    "\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"08\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_08 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"09\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_09 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"10\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_10 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"11\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_11 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"12\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_12 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"13\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_13 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"14\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_14 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"15\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_15= pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"16\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_16 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"17\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_17 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"18\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_18 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"19\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_19 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"20\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_20 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"21\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_21 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"22\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_22 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"23\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_23 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"24\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_24 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"25\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_25 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"26\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_26 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"27\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_27 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"28\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_28 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"29\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_29 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"30\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_30 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"31\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_31 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"32\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_32 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most used nodes\n",
    "\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"33\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_33 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"34\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_34 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"35\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_35 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"36\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_36 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"37\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_37 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"38\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_38 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"39\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_39 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"40\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_40 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"41\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_41 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"42\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_42 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"44\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_44 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"45\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_45 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"46\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_46 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"47\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_47 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"48\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_48 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"49\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_49 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"50\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_50 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"51\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_51 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"52\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_52 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"53\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_53 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"54\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_54 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"55\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_55 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"56\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_56 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"57\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_57 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"58\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_58 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"59\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_59 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"60\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_60 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"61\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_61 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"62\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_62 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"63\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_63 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"64\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_64 = pd.read_csv(infile_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO I probably have to load more than one node measurements file\n",
    "def calculate_job_consumption_2(job_id_string):\n",
    "    \n",
    "    job_real_pow_consumption = 0\n",
    "    job_runned_alone = True\n",
    "    job_pow_quality = 1.0\n",
    "    job_good_nodes = 0 # node where we have at most 1 measurement in the interval\n",
    "    job_measurements_tot = 0\n",
    "    job_measurements_missing_tot = 0\n",
    "    \n",
    "    partial_pow_consumption = 0\n",
    "    \n",
    "    job_to_nodes = merged_jobs_to_nodes_long[merged_jobs_to_nodes_long['job_id_string'] == job_id_string]\n",
    "    job_row = job_to_nodes.iloc[0]\n",
    "    job_to_nodes_unique = job_to_nodes.node_id.unique() # series with the nodes id\n",
    "    ### print some jobs info\n",
    "    print(\"job {} use {} cores in this node\".format(job_id_string, str(job_row['ncpus'])))\n",
    "    print(\"job {} start at {} and end at {}\".format(job_id_string, str(job_row['run_start_time']), str(job_row['end_time'])))\n",
    "    print(\"job {} requires {} nodes and {} cores\".format(job_id_string, str(job_row['node_req']), str(job_row['cpu_req'])))\n",
    "    print(job_to_nodes_unique)\n",
    "    \n",
    "    job_n_2_1 = 0\n",
    "    job_n_2_2 = 0\n",
    "    job_n_3_1 = 0\n",
    "    \n",
    "    for index, row in job_to_nodes.iterrows():\n",
    "        \n",
    "        node_type = nodes_data.iloc[row['node_id']-1]['cpu_type']\n",
    "        if(node_type == '2_1_ghz'):\n",
    "            job_n_2_1 += 1\n",
    "        elif(node_type == '2_2_ghz'):\n",
    "            job_n_2_2 += 1\n",
    "        elif(node_type == '3_1_ghz'):\n",
    "            job_n_3_1 += 1\n",
    "        \n",
    "        node_idle_core = nodes_data.iloc[row['node_id']-1]['core_idle']\n",
    "        \n",
    "        if(row['node_id'] == 1):\n",
    "            node_measurements = node_measurements_01\n",
    "        elif(row['node_id'] == 2):\n",
    "            node_measurements = node_measurements_02\n",
    "        elif(row['node_id'] == 3):\n",
    "            node_measurements = node_measurements_03\n",
    "        elif(row['node_id'] == 4):\n",
    "            node_measurements = node_measurements_04\n",
    "        elif(row['node_id'] == 5):\n",
    "            node_measurements = node_measurements_05\n",
    "        elif(row['node_id'] == 6):\n",
    "            node_measurements = node_measurements_06\n",
    "        elif(row['node_id'] == 7):\n",
    "            node_measurements = node_measurements_07\n",
    "        elif(row['node_id'] == 8):\n",
    "            node_measurements = node_measurements_08\n",
    "        elif(row['node_id'] == 9):\n",
    "            node_measurements = node_measurements_09\n",
    "        elif(row['node_id'] == 10):\n",
    "            node_measurements = node_measurements_10\n",
    "        elif(row['node_id'] == 11):\n",
    "            node_measurements = node_measurements_11\n",
    "        elif(row['node_id'] == 12):\n",
    "            node_measurements = node_measurements_12\n",
    "        elif(row['node_id'] == 13):\n",
    "            node_measurements = node_measurements_13\n",
    "        elif(row['node_id'] == 14):\n",
    "            node_measurements = node_measurements_14\n",
    "        elif(row['node_id'] == 15):\n",
    "            node_measurements = node_measurements_15\n",
    "        elif(row['node_id'] == 16):\n",
    "            node_measurements = node_measurements_16\n",
    "        elif(row['node_id'] == 17):\n",
    "            node_measurements = node_measurements_17\n",
    "        elif(row['node_id'] == 18):\n",
    "            node_measurements = node_measurements_18\n",
    "        elif(row['node_id'] == 19):\n",
    "            node_measurements = node_measurements_19\n",
    "        elif(row['node_id'] == 20):\n",
    "            node_measurements = node_measurements_20\n",
    "        elif(row['node_id'] == 21):\n",
    "            node_measurements = node_measurements_21\n",
    "        elif(row['node_id'] == 22):\n",
    "            node_measurements = node_measurements_22\n",
    "        elif(row['node_id'] == 23):\n",
    "            node_measurements = node_measurements_23\n",
    "        elif(row['node_id'] == 24):\n",
    "            node_measurements = node_measurements_24\n",
    "        elif(row['node_id'] == 25):\n",
    "            node_measurements = node_measurements_25\n",
    "        elif(row['node_id'] == 26):\n",
    "            node_measurements = node_measurements_26\n",
    "        elif(row['node_id'] == 27):\n",
    "            node_measurements = node_measurements_27\n",
    "        elif(row['node_id'] == 28):\n",
    "            node_measurements = node_measurements_28\n",
    "        elif(row['node_id'] == 29):\n",
    "            node_measurements = node_measurements_29\n",
    "        elif(row['node_id'] == 30):\n",
    "            node_measurements = node_measurements_30\n",
    "        elif(row['node_id'] == 31):\n",
    "            node_measurements = node_measurements_31\n",
    "        elif(row['node_id'] == 32):\n",
    "            node_measurements = node_measurements_32\n",
    "        elif(row['node_id'] == 33):\n",
    "            node_measurements = node_measurements_33\n",
    "        elif(row['node_id'] == 34):\n",
    "            node_measurements = node_measurements_34\n",
    "        elif(row['node_id'] == 35):\n",
    "            node_measurements = node_measurements_35\n",
    "        elif(row['node_id'] == 36):\n",
    "            node_measurements = node_measurements_36\n",
    "        elif(row['node_id'] == 37):\n",
    "            node_measurements = node_measurements_37\n",
    "        elif(row['node_id'] == 38):\n",
    "            node_measurements = node_measurements_38\n",
    "        elif(row['node_id'] == 39):\n",
    "            node_measurements = node_measurements_39\n",
    "        elif(row['node_id'] == 40):\n",
    "            node_measurements = node_measurements_40\n",
    "        elif(row['node_id'] == 41):\n",
    "            node_measurements = node_measurements_41\n",
    "        elif(row['node_id'] == 42):\n",
    "            node_measurements = node_measurements_42\n",
    "        elif(row['node_id'] == 44):\n",
    "            node_measurements = node_measurements_44\n",
    "        elif(row['node_id'] == 45):\n",
    "            node_measurements = node_measurements_45\n",
    "        elif(row['node_id'] == 46):\n",
    "            node_measurements = node_measurements_46\n",
    "        elif(row['node_id'] == 47):\n",
    "            node_measurements = node_measurements_47\n",
    "        elif(row['node_id'] == 48):\n",
    "            node_measurements = node_measurements_48\n",
    "        elif(row['node_id'] == 49):\n",
    "            node_measurements = node_measurements_49\n",
    "        elif(row['node_id'] == 50):\n",
    "            node_measurements = node_measurements_50\n",
    "        elif(row['node_id'] == 51):\n",
    "            node_measurements = node_measurements_51\n",
    "        elif(row['node_id'] == 52):\n",
    "            node_measurements = node_measurements_52\n",
    "        elif(row['node_id'] == 53):\n",
    "            node_measurements = node_measurements_53\n",
    "        elif(row['node_id'] == 54):\n",
    "            node_measurements = node_measurements_54\n",
    "        elif(row['node_id'] == 55):\n",
    "            node_measurements = node_measurements_55\n",
    "        elif(row['node_id'] == 56):\n",
    "            node_measurements = node_measurements_56\n",
    "        elif(row['node_id'] == 57):\n",
    "            node_measurements = node_measurements_57\n",
    "        elif(row['node_id'] == 58):\n",
    "            node_measurements = node_measurements_58\n",
    "        elif(row['node_id'] == 59):\n",
    "            node_measurements = node_measurements_59\n",
    "        elif(row['node_id'] == 60):\n",
    "            node_measurements = node_measurements_60\n",
    "        elif(row['node_id'] == 61):\n",
    "            node_measurements = node_measurements_61\n",
    "        elif(row['node_id'] == 62):\n",
    "            node_measurements = node_measurements_62\n",
    "        elif(row['node_id'] == 63):\n",
    "            node_measurements = node_measurements_63\n",
    "        elif(row['node_id'] == 64):\n",
    "            node_measurements = node_measurements_64\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            ### add a 0 to the number if is less than 10 (\"1\" -> \"01\", ...)\n",
    "            node_str = \"0\" + str(row['node_id']) if row['node_id'] < 10 else str(row['node_id'])\n",
    "            print(\"node {}\".format(node_str))\n",
    "            ### open the measurements node file I/O\n",
    "            infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node_str + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "            node_measurements = pd.read_csv(infile_node)\n",
    "    \n",
    "        ### take only the entries of the interval where the job was running\n",
    "        interval_measurements = node_measurements.loc[(pd.to_datetime(node_measurements['timestamp']) >= pd.to_datetime(job_row['run_start_time'])) & (pd.to_datetime(node_measurements['timestamp']) <= pd.to_datetime((job_row['end_time'])) - np.timedelta64(5, 's'))]\n",
    "        \n",
    "        ### TODO drop missing measurements (calculate the percentage)\n",
    "        n_node_measurements = interval_measurements.shape[0]\n",
    "        job_measurements_tot += n_node_measurements\n",
    "        \n",
    "        interval_measurements = interval_measurements.dropna()\n",
    "        # print(\"interval_measurements {}\".format(interval_measurements.shape[0]))\n",
    "        print(\"{}/{} intervals missing\".format(n_node_measurements-interval_measurements.shape[0], n_node_measurements))\n",
    "        job_measurements_missing_tot += n_node_measurements-interval_measurements.shape[0]\n",
    "        \n",
    "        # print(interval_measurements)\n",
    "        ### drop where active cores is greater than 16 - only on node 30\n",
    "        interval_measurements = interval_measurements[interval_measurements['active_cores'] <= 16]\n",
    "        ### TODO drop row with 0 active_cores (or less than the current job used cores)\n",
    "        interval_measurements = interval_measurements[interval_measurements['active_cores'] >= row['ncpus']]  \n",
    "        \n",
    "        if(interval_measurements.shape[0] != 0):\n",
    "            job_good_nodes += 1\n",
    "        \n",
    "        ### group the intervals wehere the partial_pow_cons can be obtaied with the same instance of the formula\n",
    "        ### take the mean for the pow columns\n",
    "        # interval_grouped = interval_measurements.reset_index().groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"]).mean()\n",
    "        interval_grouped = interval_measurements.groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"])\n",
    "        counts = interval_grouped.size().to_frame(name='counts')\n",
    "        interval_grouped = (counts\n",
    "         .join(interval_grouped.agg({'pow_tot':'mean'}).rename(columns={'pow_tot': 'pow_tot_mean'}))\n",
    "         .reset_index()\n",
    "        )\n",
    "        interval_grouped.sort_values('active_jobs')\n",
    "        \n",
    "        if(interval_grouped.shape[0]>1):\n",
    "            job_runned_alone = False\n",
    "        \n",
    "        ### apply the formula on each grouped data\n",
    "        #for index_group, row_group in interval_grouped.iterrows():\n",
    "            # n_active_cores = index_group[0]\n",
    "        #    n_active_cores = row_group['active_cores']\n",
    "        #    partial_pow_consumption += (row_group['pow_tot_0_mean'] + row_group['pow_tot_1_mean']) * row['ncpus'] / n_active_cores\n",
    "        \n",
    "        if(interval_grouped.shape[0] != 0):\n",
    "            #print(interval_grouped)\n",
    "            #interval_grouped['pow_tot'] = (interval_grouped['pow_tot_0_mean'] + interval_grouped['pow_tot_1_mean']) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "            interval_grouped['pow_tot'] = (interval_grouped['pow_tot_mean'] - (16 - interval_grouped['active_cores'])*node_idle_core) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "            \n",
    "            # not_alone_counts = interval_grouped['counts'].loc[interval_grouped['active_jobs'] != 1].sum()\n",
    "            # interval_grouped.loc[interval_grouped['active_jobs'] == 1, ['counts']] += not_alone_counts\n",
    "            print(interval_grouped)\n",
    "            partial_pow_consumption = np.average(interval_grouped['pow_tot'], weights=interval_grouped['counts'])\n",
    "            # partial_pow_consumption = partial_pow_consumption / interval_grouped.shape[0]\n",
    "            # print(interval_grouped)\n",
    "            \n",
    "        print(\"partial measurement: {}\".format(partial_pow_consumption))\n",
    "        job_real_pow_consumption += partial_pow_consumption\n",
    "        partial_pow_consumption = 0\n",
    "    \n",
    "    print(\"{}/{} node measurements are good\".format(job_good_nodes, job_to_nodes_unique.shape[0]))\n",
    "  \n",
    "    # print(\"job_real_pow_consumption is {}\".format(job_real_pow_consumption))\n",
    "    ### TODO if some entire node data is missing, approximate the amount to add\n",
    "    if(job_good_nodes != 0):\n",
    "        job_real_pow_consumption = job_real_pow_consumption + (job_real_pow_consumption / job_good_nodes) * (job_to_nodes_unique.shape[0] - job_good_nodes)\n",
    "    else:\n",
    "        job_real_pow_consumption = 8.75 * row['cpu_req']\n",
    "        job_pow_quality = 0.0\n",
    "    print(job_real_pow_consumption)  \n",
    "    \n",
    "    if(job_measurements_missing_tot != 0):\n",
    "        job_pow_quality = 1 - (job_measurements_missing_tot / job_measurements_tot)\n",
    "        \n",
    "    return job_real_pow_consumption, job_runned_alone, job_pow_quality, job_n_2_1, job_n_2_2, job_n_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "short_jobs['real_pow'] = 0.0\n",
    "short_jobs['runned_alone'] = True\n",
    "short_jobs['real_pow_quality'] = 0.0\n",
    "\n",
    "short_jobs['n_2_1'] = 0\n",
    "short_jobs['n_2_2'] = 0\n",
    "short_jobs['n_3_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206634\n",
      "94545\n"
     ]
    }
   ],
   "source": [
    "infile = datadir + 'CPUs/' + interval_comment + \"/short_jobs_real_pow.csv\"\n",
    "short_jobs_to_compute = pd.read_csv(infile, index_col=0)\n",
    "print(short_jobs_to_compute.shape[0])\n",
    "\n",
    "short_jobs_to_compute = short_jobs_to_compute[short_jobs_to_compute['real_pow'] == 0.0]\n",
    "print(short_jobs_to_compute.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the real power consumption - short jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datadir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-99f7342a1089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'CPUs/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterval_comment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/short_jobs_real_pow.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshort_jobs_to_compute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#for index, row in large_jobs_tail.iterrows():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datadir' is not defined"
     ]
    }
   ],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/short_jobs_real_pow.csv\"\n",
    "for index, row in short_jobs_to_compute.iterrows():\n",
    "#for index, row in large_jobs_tail.iterrows():\n",
    "    print()\n",
    "    print(\"{}/{}\".format(i, short_jobs_to_compute.shape[0]))\n",
    "\n",
    "    #real_pow[i], runned_alone[i], quality[i] = calculate_job_consumption(row['job_id_string'])\n",
    "    real_pow_val, runned_alone_val, quality_val, job_n_2_1, job_n_2_2, job_n_3_1 = calculate_job_consumption_short(row['job_id_string'])\n",
    "    short_jobs.at[index,'real_pow'] = real_pow_val\n",
    "    short_jobs.at[index,'runned_alone'] = runned_alone_val\n",
    "    short_jobs.at[index,'real_pow_quality'] = quality_val\n",
    "    \n",
    "    short_jobs.at[index,'n_2_1'] = job_n_2_1\n",
    "    short_jobs.at[index,'n_2_2'] = job_n_2_2\n",
    "    short_jobs.at[index,'n_3_1'] = job_n_3_1\n",
    "    \n",
    "    i = i + 1\n",
    "    if(i % 1000 == 0):\n",
    "        short_jobs.to_csv(outfile)\n",
    "        \n",
    "    #    break\n",
    "\n",
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "short_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO I probably have to load more than one node measurements file\n",
    "def calculate_job_consumption_short(job_id_string):\n",
    "    \n",
    "    job_real_pow_consumption = 0\n",
    "    job_runned_alone = True\n",
    "    job_pow_quality = 1.0\n",
    "    job_good_nodes = 0 # node where we have at most 1 measurement in the interval\n",
    "    job_measurements_tot = 0\n",
    "    job_measurements_missing_tot = 0\n",
    "    \n",
    "    partial_pow_consumption = 0\n",
    "    \n",
    "    job_to_nodes = merged_jobs_to_nodes_short[merged_jobs_to_nodes_short['job_id_string'] == job_id_string]\n",
    "    job_row = job_to_nodes.iloc[0]\n",
    "    job_to_nodes_unique = job_to_nodes.node_id.unique() # series with the nodes id\n",
    "    ### print some jobs info\n",
    "    print(\"job {} use {} cores in this node\".format(job_id_string, str(job_row['ncpus'])))\n",
    "    #print(\"job {} start at {} and end at {}\".format(job_id_string, str(job_row['run_start_time']), str(job_row['end_time'])))\n",
    "    print(\"job {} requires {} nodes and {} cores\".format(job_id_string, str(job_row['node_req']), str(job_row['cpu_req'])))\n",
    "    print(job_to_nodes_unique)\n",
    "    \n",
    "    job_n_2_1 = 0\n",
    "    job_n_2_2 = 0\n",
    "    job_n_3_1 = 0\n",
    "    \n",
    "    for index, row in job_to_nodes.iterrows():\n",
    "        \n",
    "        node_type = nodes_data.iloc[row['node_id']-1]['cpu_type']\n",
    "        if(node_type == '2_1_ghz'):\n",
    "            job_n_2_1 += 1\n",
    "        elif(node_type == '2_2_ghz'):\n",
    "            job_n_2_2 += 1\n",
    "        elif(node_type == '3_1_ghz'):\n",
    "            job_n_3_1 += 1\n",
    "        \n",
    "        node_idle_core = nodes_data.iloc[row['node_id']-1]['core_idle']\n",
    "        \n",
    "        if(row['node_id'] == 1):\n",
    "            node_measurements = node_measurements_01\n",
    "        elif(row['node_id'] == 2):\n",
    "            node_measurements = node_measurements_02\n",
    "        elif(row['node_id'] == 3):\n",
    "            node_measurements = node_measurements_03\n",
    "        elif(row['node_id'] == 4):\n",
    "            node_measurements = node_measurements_04\n",
    "        elif(row['node_id'] == 5):\n",
    "            node_measurements = node_measurements_05\n",
    "        elif(row['node_id'] == 6):\n",
    "            node_measurements = node_measurements_06\n",
    "        elif(row['node_id'] == 7):\n",
    "            node_measurements = node_measurements_07\n",
    "        elif(row['node_id'] == 8):\n",
    "            node_measurements = node_measurements_08\n",
    "        elif(row['node_id'] == 9):\n",
    "            node_measurements = node_measurements_09\n",
    "        elif(row['node_id'] == 10):\n",
    "            node_measurements = node_measurements_10\n",
    "        elif(row['node_id'] == 11):\n",
    "            node_measurements = node_measurements_11\n",
    "        elif(row['node_id'] == 12):\n",
    "            node_measurements = node_measurements_12\n",
    "        elif(row['node_id'] == 13):\n",
    "            node_measurements = node_measurements_13\n",
    "        elif(row['node_id'] == 14):\n",
    "            node_measurements = node_measurements_14\n",
    "        elif(row['node_id'] == 15):\n",
    "            node_measurements = node_measurements_15\n",
    "        elif(row['node_id'] == 16):\n",
    "            node_measurements = node_measurements_16\n",
    "        elif(row['node_id'] == 17):\n",
    "            node_measurements = node_measurements_17\n",
    "        elif(row['node_id'] == 18):\n",
    "            node_measurements = node_measurements_18\n",
    "        elif(row['node_id'] == 19):\n",
    "            node_measurements = node_measurements_19\n",
    "        elif(row['node_id'] == 20):\n",
    "            node_measurements = node_measurements_20\n",
    "        elif(row['node_id'] == 21):\n",
    "            node_measurements = node_measurements_21\n",
    "        elif(row['node_id'] == 22):\n",
    "            node_measurements = node_measurements_22\n",
    "        elif(row['node_id'] == 23):\n",
    "            node_measurements = node_measurements_23\n",
    "        elif(row['node_id'] == 24):\n",
    "            node_measurements = node_measurements_24\n",
    "        elif(row['node_id'] == 25):\n",
    "            node_measurements = node_measurements_25\n",
    "        elif(row['node_id'] == 26):\n",
    "            node_measurements = node_measurements_26\n",
    "        elif(row['node_id'] == 27):\n",
    "            node_measurements = node_measurements_27\n",
    "        elif(row['node_id'] == 28):\n",
    "            node_measurements = node_measurements_28\n",
    "        elif(row['node_id'] == 29):\n",
    "            node_measurements = node_measurements_29\n",
    "        elif(row['node_id'] == 30):\n",
    "            node_measurements = node_measurements_30\n",
    "        elif(row['node_id'] == 31):\n",
    "            node_measurements = node_measurements_31\n",
    "        elif(row['node_id'] == 32):\n",
    "            node_measurements = node_measurements_32\n",
    "        elif(row['node_id'] == 33):\n",
    "            node_measurements = node_measurements_33\n",
    "        elif(row['node_id'] == 34):\n",
    "            node_measurements = node_measurements_34\n",
    "        elif(row['node_id'] == 35):\n",
    "            node_measurements = node_measurements_35\n",
    "        elif(row['node_id'] == 36):\n",
    "            node_measurements = node_measurements_36\n",
    "        elif(row['node_id'] == 37):\n",
    "            node_measurements = node_measurements_37\n",
    "        elif(row['node_id'] == 38):\n",
    "            node_measurements = node_measurements_38\n",
    "        elif(row['node_id'] == 39):\n",
    "            node_measurements = node_measurements_39\n",
    "        elif(row['node_id'] == 40):\n",
    "            node_measurements = node_measurements_40\n",
    "        elif(row['node_id'] == 41):\n",
    "            node_measurements = node_measurements_41\n",
    "        elif(row['node_id'] == 42):\n",
    "            node_measurements = node_measurements_42\n",
    "        elif(row['node_id'] == 44):\n",
    "            node_measurements = node_measurements_44\n",
    "        elif(row['node_id'] == 45):\n",
    "            node_measurements = node_measurements_45\n",
    "        elif(row['node_id'] == 46):\n",
    "            node_measurements = node_measurements_46\n",
    "        elif(row['node_id'] == 47):\n",
    "            node_measurements = node_measurements_47\n",
    "        elif(row['node_id'] == 48):\n",
    "            node_measurements = node_measurements_48\n",
    "        elif(row['node_id'] == 49):\n",
    "            node_measurements = node_measurements_49\n",
    "        elif(row['node_id'] == 50):\n",
    "            node_measurements = node_measurements_50\n",
    "        elif(row['node_id'] == 51):\n",
    "            node_measurements = node_measurements_51\n",
    "        elif(row['node_id'] == 52):\n",
    "            node_measurements = node_measurements_52\n",
    "        elif(row['node_id'] == 53):\n",
    "            node_measurements = node_measurements_53\n",
    "        elif(row['node_id'] == 54):\n",
    "            node_measurements = node_measurements_54\n",
    "        elif(row['node_id'] == 55):\n",
    "            node_measurements = node_measurements_55\n",
    "        elif(row['node_id'] == 56):\n",
    "            node_measurements = node_measurements_56\n",
    "        elif(row['node_id'] == 57):\n",
    "            node_measurements = node_measurements_57\n",
    "        elif(row['node_id'] == 58):\n",
    "            node_measurements = node_measurements_58\n",
    "        elif(row['node_id'] == 59):\n",
    "            node_measurements = node_measurements_59\n",
    "        elif(row['node_id'] == 60):\n",
    "            node_measurements = node_measurements_60\n",
    "        elif(row['node_id'] == 61):\n",
    "            node_measurements = node_measurements_61\n",
    "        elif(row['node_id'] == 62):\n",
    "            node_measurements = node_measurements_62\n",
    "        elif(row['node_id'] == 63):\n",
    "            node_measurements = node_measurements_63\n",
    "        elif(row['node_id'] == 64):\n",
    "            node_measurements = node_measurements_64\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            ### add a 0 to the number if is less than 10 (\"1\" -> \"01\", ...)\n",
    "            node_str = \"0\" + str(row['node_id']) if row['node_id'] < 10 else str(row['node_id'])\n",
    "            print(\"node {}\".format(node_str))\n",
    "            ### open the measurements node file I/O\n",
    "            infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node_str + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "            node_measurements = pd.read_csv(infile_node)\n",
    "    \n",
    "    \n",
    "        measure = node_measurements[node_measurements['timestamp'] == job_row['run_start_time'][:-3]+':00']\n",
    "        \n",
    "        if(measure.shape[0] != 0):\n",
    "            if(measure['active_cores'].iloc[0] != 0):\n",
    "                job_runned_alone = False\n",
    "            \n",
    "            active_cores = measure['active_cores'].iloc[0] + row['ncpus']\n",
    "            if(active_cores > 16):\n",
    "                active_cores = 16\n",
    "            job_real_pow_consumption = job_real_pow_consumption + (measure['pow_tot_0'].iloc[0] + measure['pow_tot_1'].iloc[0] - (16 - active_cores) * node_idle_core) * row['ncpus'] / active_cores\n",
    "        else:\n",
    "            print(\"error measure\")\n",
    "            job_pow_quality = 0.0\n",
    "    \n",
    "    if(job_real_pow_consumption < 0):\n",
    "        job_real_pow_consumption = 0\n",
    "        job_pow_quality = 0.0\n",
    "    print(job_real_pow_consumption)\n",
    "        \n",
    "    return job_real_pow_consumption, job_runned_alone, job_pow_quality, job_n_2_1, job_n_2_2, job_n_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id_string: 500899.node129\n"
     ]
    }
   ],
   "source": [
    "#### calculate the power consumption of a specifified job\n",
    "\n",
    "\n",
    "\n",
    "job_id_string = \"498462.node129\" # non ha misurazioni\n",
    "job_id_string = \"498463.node129\" # non ha misurazioni\n",
    "job_id_string = \"498464.node129\" # non ha misurazioni\n",
    "job_id_string = \"498465.node129\" # [ 35]\n",
    "\n",
    "job_id_string = \"498699.node129\" # [ 37]\n",
    "job_id_string = \"498458.node129\" # non ha misurazioni\n",
    "job_id_string = \"498458.node129\" # non ha misurazioni\n",
    "job_id_string = \"498461.node129\" # single node 16 cores [ 27]\n",
    "job_id_string = \"500752.node129\" # 2 nodi, 24 cores [ 16 20] 20 totalmente assente\n",
    "job_id_string = \"500899.node129\" # [ 15 23 32 28 25 19]\n",
    "\n",
    "job_id_string = \"498467.node129\"\n",
    "\n",
    "job_id_string = \"498460.node129\" # 6 nodi, 96 cores [ 9 28 25 19 26 29]\n",
    "job_id_string = \"498466.node129\" # [ 36 38 40 42 50 56 59 61 62 63 54]\n",
    "job_id_string = \"500752.node129\" # 2 nodi, 24 cores [ 16 20] 20 totalmente assente\n",
    "job_id_string = \"498459.node129\" # single node 16 cores [ 9]\n",
    "job_id_string = \"498699.node129\" # [ 37]\n",
    "job_id_string = \"498458.node129\" # [ ]\n",
    "job_id_string = \"498460.node129\" # 6 nodi, 96 cores [ 9 28 25 19 26 29]\n",
    "job_id_string = \"500899.node129\" # [ 15 23 32 28 25 19]\n",
    "\n",
    "\n",
    "print(\"job_id_string: {}\".format(job_id_string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 500899.node129 use 16 cores in this node\n",
      "job 500899.node129 start at 2014-03-31 19:24:06 and end at 2014-03-31 19:40:33\n",
      "job 500899.node129 requires 6 nodes and 96 cores\n",
      "[15 23 32 28 25 19]\n",
      "node 15\n",
      "0/15 intervals missing\n",
      "   active_cores  active_jobs  active_gpus  active_mics  counts  pow_tot_0_mean  pow_tot_1_mean\n",
      "0            16            1            0            0      15       94.476258         96.9878\n",
      "partial measurement: 191.46405847759212\n",
      "0/15 intervals missing\n",
      "   active_cores  active_jobs  active_gpus  active_mics  counts  pow_tot_0_mean  pow_tot_1_mean\n",
      "0            16            1            0            0      15       93.407682       92.983533\n",
      "partial measurement: 186.3912153826789\n",
      "node 32\n",
      "15/15 intervals missing\n",
      "partial measurement: 0\n",
      "0/15 intervals missing\n",
      "   active_cores  active_jobs  active_gpus  active_mics  counts  pow_tot_0_mean  pow_tot_1_mean\n",
      "0            16            1            0            0      15       97.068548       98.201442\n",
      "partial measurement: 195.26999001743422\n",
      "0/15 intervals missing\n",
      "   active_cores  active_jobs  active_gpus  active_mics  counts  pow_tot_0_mean  pow_tot_1_mean\n",
      "0            16            1            0            0      15       94.655195       96.648039\n",
      "partial measurement: 191.30323451805947\n",
      "0/15 intervals missing\n",
      "   active_cores  active_jobs  active_gpus  active_mics  counts  pow_tot_0_mean  pow_tot_1_mean\n",
      "0            16            1            0            0      15       94.005422        97.82748\n",
      "partial measurement: 191.83290255528487\n",
      "5/6 node measurements are good\n",
      "1147.5136811412594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1147.5136811412594, True, 0.8333333333333334, 4, 2, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_job_consumption_2(job_id_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
