{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import errno    \n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Author: Enrico Ceccolini\n",
    "    TODO write the description\n",
    "'''\n",
    "\n",
    "#datadir = \"/datasets/eurora_data/db1/\"\n",
    "datadir = \"/datasets/eurora_data/db/\"\n",
    "infile_jobs_to_nodes = datadir + \"job_nodes.csv\"\n",
    "\n",
    "\n",
    "infile_nodes = datadir + \"nodes.csv\"\n",
    "\n",
    "suffix = \"_5sec_\"\n",
    "### select an interval from\n",
    "## 1 settings wholeData\n",
    "interval_comment_whole = \"WholeData\"\n",
    "\n",
    "### select an interval from\n",
    "## 2 settings Andrea\n",
    "#interval_comment = \"Andrea\"\n",
    "#start_time = pd.to_datetime('2014-03-31')\n",
    "#end_time = pd.to_datetime('2014-05-01')\n",
    "#infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "## 3 settings Alina\n",
    "interval_comment = \"Alina\"\n",
    "start_time = pd.to_datetime('2014-06-30')\n",
    "end_time = pd.to_datetime('2014-11-01')\n",
    "infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "nodes=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the nodes we have three differentIntel Xeon E5 processors stepping: nodes 1-16 & 25-32 havea maximum frequency of 2.1GHz (CPUs-2100), nodes 17-24 have a maximum frequency of 2.2GHz (CPUs-2200), andnodes 33-64 have a maximum frequency of 3.1GHz (CPUs-3100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.read_csv(infile_nodes, index_col=0)\n",
    "# nodes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read job2nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_to_nodes_whole_data = pd.read_csv(infile_jobs_to_nodes, index_col=0)\n",
    "print(\"jobs_to_nodes_whole_data contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of jobs runned on node 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data[jobs_to_nodes_whole_data['node_id'] != 129]\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of duplicates job_id_string, node_id\n",
    "this is probably caused by the concurrency writing of the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data.drop_duplicates(subset=['job_id_string', 'node_id'])\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read jobs data\n",
    "\n",
    "drop the jobs out of interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_whole = pd.read_csv(infile_jobs + \".csv\", index_col=0)\n",
    "print(\"jobs_whole_data contains {} records\".format(jobs_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_jobs_to_nodes = pd.merge(jobs_to_nodes_whole_data, jobs_whole, on='job_id_string')\n",
    "print(merged_jobs_to_nodes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "jobs_whole['real_pow'] = 0.0\n",
    "jobs_whole['ran_alone'] = True\n",
    "jobs_whole['real_pow_quality'] = 0.0\n",
    "\n",
    "jobs_whole['n_2_1'] = 0\n",
    "jobs_whole['n_2_2'] = 0\n",
    "jobs_whole['n_3_1'] = 0\n",
    "\n",
    "jobs_whole['job_tot_timepoints'] = 0\n",
    "jobs_whole['job_timepoints'] = 0\n",
    "jobs_whole['good_nodes'] = 0 # TODO add this on the file? maybe I can drop it at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#debug\n",
    "#jobs_whole = jobs_whole.head(15)\n",
    "#debug_merged_jobs_to_nodes = merged_jobs_to_nodes.head(1000)\n",
    "#merged_jobs_to_nodes[merged_jobs_to_nodes['node_id'] == 55]['end_time']\n",
    "#nodes = ['55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svi\n",
    "#jobs_whole[jobs_whole['job_id_string'] == '1006691.node129'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow.csv\"\n",
    "\n",
    "for node in nodes:\n",
    "    print(\"------------------------------------------- nodo: {}\".format(node))\n",
    "    merged_jobs_to_node = merged_jobs_to_nodes[merged_jobs_to_nodes['node_id'] == int(node)]\n",
    "    \n",
    "    infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "    node_measurements = pd.read_csv(infile_node)\n",
    "    node_start_time = node_measurements['timestamp'].iloc[0]\n",
    "    num_minutes = node_measurements.shape[0]\n",
    "    print(\"-------------------node data starts at {}, num minute {}\".format(node_start_time, num_minutes))\n",
    "    \n",
    "    for index_job_to_node, row_job_to_node in merged_jobs_to_nodes.iterrows():\n",
    "        print()\n",
    "        print(\"{}/{}\".format(i, merged_jobs_to_nodes.shape[0]))\n",
    "        \n",
    "        partial_pow_consumption = 0\n",
    "        job_id_string = row_job_to_node['job_id_string']\n",
    "        \n",
    "        # recupero indice del job in jobs_whole\n",
    "        job = jobs_whole[jobs_whole['job_id_string'] == job_id_string]\n",
    "        job_index = jobs_whole[jobs_whole['job_id_string'] == job_id_string].index\n",
    "        if(job_index != None):\n",
    "            job_real_consumption = job['real_pow'].iloc[0]\n",
    "            job_n_2_1 = job['n_2_1'].iloc[0]\n",
    "            job_n_2_2 = job['n_2_2'].iloc[0]\n",
    "            job_n_3_1 = job['n_3_1'].iloc[0]\n",
    "\n",
    "            job_tot_timepoints = job['job_tot_timepoints'].iloc[0]\n",
    "            job_timepoints = job['job_timepoints'].iloc[0]\n",
    "            job_good_nodes = job['good_nodes'].iloc[0]\n",
    "            job_ran_alone = job['ran_alone'].iloc[0]\n",
    "\n",
    "            node_type = nodes_data.iloc[row_job_to_node['node_id']-1]['cpu_type']\n",
    "            if(node_type == '2_1_ghz'):\n",
    "                job_n_2_1 += 1\n",
    "            elif(node_type == '2_2_ghz'):\n",
    "                job_n_2_2 += 1\n",
    "            elif(node_type == '3_1_ghz'):\n",
    "                job_n_3_1 += 1\n",
    "\n",
    "            node_idle_core = nodes_data.iloc[row_job_to_node['node_id']-1]['core_idle']\n",
    "            \n",
    "            job_start_time = pd.to_datetime(row_job_to_node['run_start_time'])\n",
    "            job_end_time = pd.to_datetime(row_job_to_node['end_time'])\n",
    "            \n",
    "            if(job_end_time - job_start_time >= np.timedelta64(5, 's')):\n",
    "                before_minutes = int((job_start_time - pd.to_datetime(node_start_time)) / np.timedelta64(5, 's'))\n",
    "                running_minutes = int((job_end_time - job_start_time) / np.timedelta64(5, 's'))\n",
    "                after_minutes = num_minutes - running_minutes - before_minutes\n",
    "                \n",
    "                before_serie = pd.Series(False, index=np.arange(before_minutes))\n",
    "                running_serie = pd.Series(True, index=np.arange(running_minutes))\n",
    "                after_serie = pd.Series(False, index=np.arange(after_minutes))\n",
    "                concat_series = pd.concat([before_serie, running_serie, after_serie], ignore_index=True)\n",
    "                \n",
    "                interval_measurements = node_measurements[concat_series]\n",
    "                \n",
    "                ### take only the entries of the interval where the job was running\n",
    "                #interval_measurements = node_measurements.loc[(pd.to_datetime(node_measurements['timestamp']) >= pd.to_datetime(row_job_to_node['run_start_time'])) & (pd.to_datetime(node_measurements['timestamp']) <= pd.to_datetime((row_job_to_node['end_time'])) - np.timedelta64(5, 's'))]\n",
    "                ### drop missing measurements (calculate the percentage)\n",
    "                n_node_measurements = interval_measurements.shape[0]\n",
    "                job_tot_timepoints += n_node_measurements\n",
    "\n",
    "                interval_measurements = interval_measurements.dropna()\n",
    "                print(\"{}/{} intervals missing\".format(n_node_measurements-interval_measurements.shape[0], n_node_measurements))\n",
    "                job_timepoints += n_node_measurements-interval_measurements.shape[0]\n",
    "\n",
    "                ### drop where active cores is greater than 16 - this problem occours only once on node 30\n",
    "                interval_measurements = interval_measurements[interval_measurements['active_cores'] <= 16]\n",
    "                ### drop row with 0 active_cores (or less than the current job used cores)\n",
    "                interval_measurements = interval_measurements[interval_measurements['active_cores'] >= row_job_to_node['ncpus']]  \n",
    "\n",
    "                if(interval_measurements.shape[0] != 0):\n",
    "                    job_good_nodes += 1 \n",
    "\n",
    "\n",
    "                ### group the intervals wehere the partial_pow_cons can be obtaied with the same instance of the formula\n",
    "                ### take the mean for the pow columns\n",
    "                # interval_grouped = interval_measurements.reset_index().groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"]).mean()\n",
    "                interval_grouped = interval_measurements.groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"])\n",
    "                counts = interval_grouped.size().to_frame(name='counts')\n",
    "                interval_grouped = (counts\n",
    "                 .join(interval_grouped.agg({'pow_tot':'mean'}).rename(columns={'pow_tot': 'pow_tot_mean'}))\n",
    "                 .reset_index()\n",
    "                )\n",
    "                interval_grouped.sort_values('active_jobs')\n",
    "\n",
    "                if(interval_grouped.shape[0]>1):\n",
    "                    job_ran_alone = False\n",
    "\n",
    "\n",
    "                if(interval_grouped.shape[0] != 0):\n",
    "                    #interval_grouped['pow_tot'] = (interval_grouped['pow_tot_0_mean'] + interval_grouped['pow_tot_1_mean']) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "                    interval_grouped['pow_tot'] = (interval_grouped['pow_tot_mean'] - (16 - interval_grouped['active_cores'])*node_idle_core) * row_job_to_node['ncpus'] / interval_grouped['active_cores'] \n",
    "\n",
    "                    # not_alone_counts = interval_grouped['counts'].loc[interval_grouped['active_jobs'] != 1].sum()\n",
    "                    # interval_grouped.loc[interval_grouped['active_jobs'] == 1, ['counts']] += not_alone_counts\n",
    "                    print(interval_grouped)\n",
    "                    partial_pow_consumption = np.average(interval_grouped['pow_tot'], weights=interval_grouped['counts'])\n",
    "                    # partial_pow_consumption = partial_pow_consumption / interval_grouped.shape[0]\n",
    "                    # print(interval_grouped)\n",
    "\n",
    "                print(\"partial measurement: {}\".format(partial_pow_consumption))\n",
    "                job_real_consumption += partial_pow_consumption\n",
    "                partial_pow_consumption = 0\n",
    "\n",
    "        \n",
    "            else: # jobs shorter than 5 sec\n",
    "                   # keep the only timepoint\n",
    "                measure = node_measurements[node_measurements['timestamp'] == row_job_to_node['run_start_time'][:-3]+':00']\n",
    "                job_tot_timepoints += 1\n",
    "                if(measure.shape[0] != 0):\n",
    "                    job_timepoints += 1\n",
    "                    job_good_nodes += 1\n",
    "                    if(measure['active_cores'].iloc[0] != 0):\n",
    "                        job_ran_alone = False\n",
    "\n",
    "                    active_cores = measure['active_cores'].iloc[0] + row_job_to_node['ncpus']\n",
    "                    if(active_cores > 16):\n",
    "                        active_cores = 16\n",
    "                    job_real_consumption += (measure['pow_tot'].iloc[0] - (16 - active_cores) * node_idle_core) * row_job_to_node['ncpus'] / active_cores\n",
    "                else:\n",
    "                    print(\"error measure\")\n",
    "                    job_pow_quality = 0.0\n",
    "               \n",
    "            \n",
    "            if(job_real_consumption < 0):\n",
    "                job_real_consumption = 0.0\n",
    "            print(\"job_real_consumption: {}\".format(job_real_consumption))\n",
    "            \n",
    "            jobs_whole.at[job_index, 'real_pow'] = job_real_consumption\n",
    "            jobs_whole.at[job_index, 'n_2_1'] = job_n_2_1\n",
    "            jobs_whole.at[job_index, 'n_2_2'] = job_n_2_2\n",
    "            jobs_whole.at[job_index, 'n_3_1'] = job_n_3_1\n",
    "            jobs_whole.at[job_index, 'job_tot_timepoints'] = job_tot_timepoints\n",
    "            jobs_whole.at[job_index, 'job_timepoints'] = job_timepoints\n",
    "            jobs_whole.at[job_index, 'good_nodes'] = job_good_nodes\n",
    "            jobs_whole.at[job_index, 'ran_alone'] = job_ran_alone\n",
    "    \n",
    "        \n",
    "        i = i + 1\n",
    "        if(i % 10000 == 0):\n",
    "            jobs_whole.to_csv(outfile + \"_\" + str(i) + \".csv\")\n",
    "        \n",
    "jobs_whole.to_csv(outfile + \"_\" + str(i) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the real power consumption - long jobs (4h Andrea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "jobs_whole['real_pow'] = 0.0\n",
    "jobs_whole['runned_alone'] = True\n",
    "jobs_whole['real_pow_quality'] = 0.0\n",
    "\n",
    "jobs_whole['n_2_1'] = 0\n",
    "jobs_whole['n_2_2'] = 0\n",
    "jobs_whole['n_3_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(real_pow.shape[0])\n",
    "#print(runned_alone.shape[0])\n",
    "#print(quality.shape[0])\n",
    "\n",
    "#large_jobs_tail = large_jobs[large_jobs.index > 390345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs_tail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow.csv\"\n",
    "for index, row in jobs_whole.iterrows():\n",
    "#for index, row in large_jobs_tail.iterrows():\n",
    "    print()\n",
    "    print(\"{}/{}\".format(i, jobs_whole.shape[0]))\n",
    "\n",
    "    #real_pow[i], runned_alone[i], quality[i] = calculate_job_consumption(row['job_id_string'])\n",
    "    real_pow_val, runned_alone_val, quality_val, job_n_2_1, job_n_2_2, job_n_3_1 = calculate_job_consumption_2(row['job_id_string'])\n",
    "    jobs_whole.at[index,'real_pow'] = real_pow_val\n",
    "    jobs_whole.at[index,'runned_alone'] = runned_alone_val\n",
    "    jobs_whole.at[index,'real_pow_quality'] = quality_val\n",
    "    \n",
    "    jobs_whole.at[index,'n_2_1'] = job_n_2_1\n",
    "    jobs_whole.at[index,'n_2_2'] = job_n_2_2\n",
    "    jobs_whole.at[index,'n_3_1'] = job_n_3_1\n",
    "    \n",
    "    i = i + 1\n",
    "    if(i % 1000 == 0):\n",
    "        jobs_whole.to_csv(outfile)\n",
    "        \n",
    "    #    break\n",
    "\n",
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "jobs_whole.to_csv(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "#outfile = datadir + 'CPUs/' + interval_comment + \"/large_jobs_real_pow_3.csv\"\n",
    "#large_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"01\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_01 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"02\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_02 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"03\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_03 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"04\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_04 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"05\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_05 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"06\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_06 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"07\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_07 = pd.read_csv(infile_node)\n",
    "\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"08\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_08 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"09\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_09 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"10\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_10 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"11\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_11 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"12\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_12 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"13\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_13 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"14\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_14 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"15\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_15= pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"16\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_16 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"17\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_17 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"18\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_18 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"19\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_19 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"20\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_20 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"21\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_21 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"22\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_22 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"23\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_23 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"24\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_24 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"25\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_25 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"26\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_26 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"27\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_27 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"28\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_28 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"29\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_29 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"30\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_30 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"31\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_31 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"32\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_32 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most used nodes\n",
    "\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"33\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_33 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"34\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_34 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"35\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_35 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"36\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_36 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"37\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_37 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"38\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_38 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"39\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_39 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"40\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_40 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"41\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_41 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"42\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_42 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"44\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_44 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"45\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_45 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"46\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_46 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"47\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_47 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"48\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_48 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"49\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_49 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"50\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_50 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"51\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_51 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"52\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_52 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"53\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_53 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"54\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_54 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"55\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_55 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"56\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_56 = pd.read_csv(infile_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"57\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_57 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"58\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_58 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"59\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_59 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"60\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_60 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"61\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_61 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"62\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_62 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"63\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_63 = pd.read_csv(infile_node)\n",
    "infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + \"64\" + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "node_measurements_64 = pd.read_csv(infile_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO I probably have to load more than one node measurements file\n",
    "def calculate_job_consumption_2(job_id_string):\n",
    "    \n",
    "    job_real_pow_consumption = 0\n",
    "    job_runned_alone = True\n",
    "    job_pow_quality = 1.0\n",
    "    job_good_nodes = 0 # node where we have at most 1 measurement in the interval\n",
    "    job_measurements_tot = 0\n",
    "    job_measurements_missing_tot = 0\n",
    "    \n",
    "    partial_pow_consumption = 0\n",
    "    \n",
    "    job_to_nodes = merged_jobs_to_nodes_long[merged_jobs_to_nodes_long['job_id_string'] == job_id_string]\n",
    "    job_row = job_to_nodes.iloc[0]\n",
    "    job_to_nodes_unique = job_to_nodes.node_id.unique() # series with the nodes id\n",
    "    ### print some jobs info\n",
    "    print(\"job {} use {} cores in this node\".format(job_id_string, str(job_row['ncpus'])))\n",
    "    print(\"job {} start at {} and end at {}\".format(job_id_string, str(job_row['run_start_time']), str(job_row['end_time'])))\n",
    "    print(\"job {} requires {} nodes and {} cores\".format(job_id_string, str(job_row['node_req']), str(job_row['cpu_req'])))\n",
    "    print(job_to_nodes_unique)\n",
    "    \n",
    "    job_n_2_1 = 0\n",
    "    job_n_2_2 = 0\n",
    "    job_n_3_1 = 0\n",
    "    \n",
    "    for index, row in job_to_nodes.iterrows():\n",
    "        \n",
    "        node_type = nodes_data.iloc[row['node_id']-1]['cpu_type']\n",
    "        if(node_type == '2_1_ghz'):\n",
    "            job_n_2_1 += 1\n",
    "        elif(node_type == '2_2_ghz'):\n",
    "            job_n_2_2 += 1\n",
    "        elif(node_type == '3_1_ghz'):\n",
    "            job_n_3_1 += 1\n",
    "        \n",
    "        node_idle_core = nodes_data.iloc[row['node_id']-1]['core_idle']\n",
    "        \n",
    "        if(row['node_id'] == 1):\n",
    "            node_measurements = node_measurements_01\n",
    "        elif(row['node_id'] == 2):\n",
    "            node_measurements = node_measurements_02\n",
    "        elif(row['node_id'] == 3):\n",
    "            node_measurements = node_measurements_03\n",
    "        elif(row['node_id'] == 4):\n",
    "            node_measurements = node_measurements_04\n",
    "        elif(row['node_id'] == 5):\n",
    "            node_measurements = node_measurements_05\n",
    "        elif(row['node_id'] == 6):\n",
    "            node_measurements = node_measurements_06\n",
    "        elif(row['node_id'] == 7):\n",
    "            node_measurements = node_measurements_07\n",
    "        elif(row['node_id'] == 8):\n",
    "            node_measurements = node_measurements_08\n",
    "        elif(row['node_id'] == 9):\n",
    "            node_measurements = node_measurements_09\n",
    "        elif(row['node_id'] == 10):\n",
    "            node_measurements = node_measurements_10\n",
    "        elif(row['node_id'] == 11):\n",
    "            node_measurements = node_measurements_11\n",
    "        elif(row['node_id'] == 12):\n",
    "            node_measurements = node_measurements_12\n",
    "        elif(row['node_id'] == 13):\n",
    "            node_measurements = node_measurements_13\n",
    "        elif(row['node_id'] == 14):\n",
    "            node_measurements = node_measurements_14\n",
    "        elif(row['node_id'] == 15):\n",
    "            node_measurements = node_measurements_15\n",
    "        elif(row['node_id'] == 16):\n",
    "            node_measurements = node_measurements_16\n",
    "        elif(row['node_id'] == 17):\n",
    "            node_measurements = node_measurements_17\n",
    "        elif(row['node_id'] == 18):\n",
    "            node_measurements = node_measurements_18\n",
    "        elif(row['node_id'] == 19):\n",
    "            node_measurements = node_measurements_19\n",
    "        elif(row['node_id'] == 20):\n",
    "            node_measurements = node_measurements_20\n",
    "        elif(row['node_id'] == 21):\n",
    "            node_measurements = node_measurements_21\n",
    "        elif(row['node_id'] == 22):\n",
    "            node_measurements = node_measurements_22\n",
    "        elif(row['node_id'] == 23):\n",
    "            node_measurements = node_measurements_23\n",
    "        elif(row['node_id'] == 24):\n",
    "            node_measurements = node_measurements_24\n",
    "        elif(row['node_id'] == 25):\n",
    "            node_measurements = node_measurements_25\n",
    "        elif(row['node_id'] == 26):\n",
    "            node_measurements = node_measurements_26\n",
    "        elif(row['node_id'] == 27):\n",
    "            node_measurements = node_measurements_27\n",
    "        elif(row['node_id'] == 28):\n",
    "            node_measurements = node_measurements_28\n",
    "        elif(row['node_id'] == 29):\n",
    "            node_measurements = node_measurements_29\n",
    "        elif(row['node_id'] == 30):\n",
    "            node_measurements = node_measurements_30\n",
    "        elif(row['node_id'] == 31):\n",
    "            node_measurements = node_measurements_31\n",
    "        elif(row['node_id'] == 32):\n",
    "            node_measurements = node_measurements_32\n",
    "        elif(row['node_id'] == 33):\n",
    "            node_measurements = node_measurements_33\n",
    "        elif(row['node_id'] == 34):\n",
    "            node_measurements = node_measurements_34\n",
    "        elif(row['node_id'] == 35):\n",
    "            node_measurements = node_measurements_35\n",
    "        elif(row['node_id'] == 36):\n",
    "            node_measurements = node_measurements_36\n",
    "        elif(row['node_id'] == 37):\n",
    "            node_measurements = node_measurements_37\n",
    "        elif(row['node_id'] == 38):\n",
    "            node_measurements = node_measurements_38\n",
    "        elif(row['node_id'] == 39):\n",
    "            node_measurements = node_measurements_39\n",
    "        elif(row['node_id'] == 40):\n",
    "            node_measurements = node_measurements_40\n",
    "        elif(row['node_id'] == 41):\n",
    "            node_measurements = node_measurements_41\n",
    "        elif(row['node_id'] == 42):\n",
    "            node_measurements = node_measurements_42\n",
    "        elif(row['node_id'] == 44):\n",
    "            node_measurements = node_measurements_44\n",
    "        elif(row['node_id'] == 45):\n",
    "            node_measurements = node_measurements_45\n",
    "        elif(row['node_id'] == 46):\n",
    "            node_measurements = node_measurements_46\n",
    "        elif(row['node_id'] == 47):\n",
    "            node_measurements = node_measurements_47\n",
    "        elif(row['node_id'] == 48):\n",
    "            node_measurements = node_measurements_48\n",
    "        elif(row['node_id'] == 49):\n",
    "            node_measurements = node_measurements_49\n",
    "        elif(row['node_id'] == 50):\n",
    "            node_measurements = node_measurements_50\n",
    "        elif(row['node_id'] == 51):\n",
    "            node_measurements = node_measurements_51\n",
    "        elif(row['node_id'] == 52):\n",
    "            node_measurements = node_measurements_52\n",
    "        elif(row['node_id'] == 53):\n",
    "            node_measurements = node_measurements_53\n",
    "        elif(row['node_id'] == 54):\n",
    "            node_measurements = node_measurements_54\n",
    "        elif(row['node_id'] == 55):\n",
    "            node_measurements = node_measurements_55\n",
    "        elif(row['node_id'] == 56):\n",
    "            node_measurements = node_measurements_56\n",
    "        elif(row['node_id'] == 57):\n",
    "            node_measurements = node_measurements_57\n",
    "        elif(row['node_id'] == 58):\n",
    "            node_measurements = node_measurements_58\n",
    "        elif(row['node_id'] == 59):\n",
    "            node_measurements = node_measurements_59\n",
    "        elif(row['node_id'] == 60):\n",
    "            node_measurements = node_measurements_60\n",
    "        elif(row['node_id'] == 61):\n",
    "            node_measurements = node_measurements_61\n",
    "        elif(row['node_id'] == 62):\n",
    "            node_measurements = node_measurements_62\n",
    "        elif(row['node_id'] == 63):\n",
    "            node_measurements = node_measurements_63\n",
    "        elif(row['node_id'] == 64):\n",
    "            node_measurements = node_measurements_64\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            ### add a 0 to the number if is less than 10 (\"1\" -> \"01\", ...)\n",
    "            node_str = \"0\" + str(row['node_id']) if row['node_id'] < 10 else str(row['node_id'])\n",
    "            print(\"node {}\".format(node_str))\n",
    "            ### open the measurements node file I/O\n",
    "            infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node_str + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "            node_measurements = pd.read_csv(infile_node)\n",
    "    \n",
    "        ### take only the entries of the interval where the job was running\n",
    "        interval_measurements = node_measurements.loc[(pd.to_datetime(node_measurements['timestamp']) >= pd.to_datetime(job_row['run_start_time'])) & (pd.to_datetime(node_measurements['timestamp']) <= pd.to_datetime((job_row['end_time'])) - np.timedelta64(5, 's'))]\n",
    "        \n",
    "        ### TODO drop missing measurements (calculate the percentage)\n",
    "        n_node_measurements = interval_measurements.shape[0]\n",
    "        job_measurements_tot += n_node_measurements\n",
    "        \n",
    "        interval_measurements = interval_measurements.dropna()\n",
    "        # print(\"interval_measurements {}\".format(interval_measurements.shape[0]))\n",
    "        print(\"{}/{} intervals missing\".format(n_node_measurements-interval_measurements.shape[0], n_node_measurements))\n",
    "        job_measurements_missing_tot += n_node_measurements-interval_measurements.shape[0]\n",
    "        \n",
    "        # print(interval_measurements)\n",
    "        ### drop where active cores is greater than 16 - only on node 30\n",
    "        interval_measurements = interval_measurements[interval_measurements['active_cores'] <= 16]\n",
    "        ### TODO drop row with 0 active_cores (or less than the current job used cores)\n",
    "        interval_measurements = interval_measurements[interval_measurements['active_cores'] >= row['ncpus']]  \n",
    "        \n",
    "        if(interval_measurements.shape[0] != 0):\n",
    "            job_good_nodes += 1\n",
    "        \n",
    "        ### group the intervals wehere the partial_pow_cons can be obtaied with the same instance of the formula\n",
    "        ### take the mean for the pow columns\n",
    "        # interval_grouped = interval_measurements.reset_index().groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"]).mean()\n",
    "        interval_grouped = interval_measurements.groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"])\n",
    "        counts = interval_grouped.size().to_frame(name='counts')\n",
    "        interval_grouped = (counts\n",
    "         .join(interval_grouped.agg({'pow_tot':'mean'}).rename(columns={'pow_tot': 'pow_tot_mean'}))\n",
    "         .reset_index()\n",
    "        )\n",
    "        interval_grouped.sort_values('active_jobs')\n",
    "        \n",
    "        if(interval_grouped.shape[0]>1):\n",
    "            job_runned_alone = False\n",
    "        \n",
    "        ### apply the formula on each grouped data\n",
    "        #for index_group, row_group in interval_grouped.iterrows():\n",
    "            # n_active_cores = index_group[0]\n",
    "        #    n_active_cores = row_group['active_cores']\n",
    "        #    partial_pow_consumption += (row_group['pow_tot_0_mean'] + row_group['pow_tot_1_mean']) * row['ncpus'] / n_active_cores\n",
    "        \n",
    "        if(interval_grouped.shape[0] != 0):\n",
    "            #print(interval_grouped)\n",
    "            #interval_grouped['pow_tot'] = (interval_grouped['pow_tot_0_mean'] + interval_grouped['pow_tot_1_mean']) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "            interval_grouped['pow_tot'] = (interval_grouped['pow_tot_mean'] - (16 - interval_grouped['active_cores'])*node_idle_core) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "            \n",
    "            # not_alone_counts = interval_grouped['counts'].loc[interval_grouped['active_jobs'] != 1].sum()\n",
    "            # interval_grouped.loc[interval_grouped['active_jobs'] == 1, ['counts']] += not_alone_counts\n",
    "            print(interval_grouped)\n",
    "            partial_pow_consumption = np.average(interval_grouped['pow_tot'], weights=interval_grouped['counts'])\n",
    "            # partial_pow_consumption = partial_pow_consumption / interval_grouped.shape[0]\n",
    "            # print(interval_grouped)\n",
    "            \n",
    "        print(\"partial measurement: {}\".format(partial_pow_consumption))\n",
    "        job_real_pow_consumption += partial_pow_consumption\n",
    "        partial_pow_consumption = 0\n",
    "    \n",
    "    print(\"{}/{} node measurements are good\".format(job_good_nodes, job_to_nodes_unique.shape[0]))\n",
    "  \n",
    "    # print(\"job_real_pow_consumption is {}\".format(job_real_pow_consumption))\n",
    "    ### TODO if some entire node data is missing, approximate the amount to add\n",
    "    if(job_good_nodes != 0):\n",
    "        job_real_pow_consumption = job_real_pow_consumption + (job_real_pow_consumption / job_good_nodes) * (job_to_nodes_unique.shape[0] - job_good_nodes)\n",
    "    else:\n",
    "        job_real_pow_consumption = 8.75 * row['cpu_req']\n",
    "        job_pow_quality = 0.0\n",
    "    print(job_real_pow_consumption)  \n",
    "    \n",
    "    if(job_measurements_missing_tot != 0):\n",
    "        job_pow_quality = 1 - (job_measurements_missing_tot / job_measurements_tot)\n",
    "        \n",
    "    return job_real_pow_consumption, job_runned_alone, job_pow_quality, job_n_2_1, job_n_2_2, job_n_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "short_jobs['real_pow'] = 0.0\n",
    "short_jobs['runned_alone'] = True\n",
    "short_jobs['real_pow_quality'] = 0.0\n",
    "\n",
    "short_jobs['n_2_1'] = 0\n",
    "short_jobs['n_2_2'] = 0\n",
    "short_jobs['n_3_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = datadir + 'CPUs/' + interval_comment + \"/short_jobs_real_pow.csv\"\n",
    "short_jobs_to_compute = pd.read_csv(infile, index_col=0)\n",
    "print(short_jobs_to_compute.shape[0])\n",
    "\n",
    "short_jobs_to_compute = short_jobs_to_compute[short_jobs_to_compute['real_pow'] == 0.0]\n",
    "print(short_jobs_to_compute.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the real power consumption - short jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/short_jobs_real_pow.csv\"\n",
    "for index, row in short_jobs_to_compute.iterrows():\n",
    "#for index, row in large_jobs_tail.iterrows():\n",
    "    print()\n",
    "    print(\"{}/{}\".format(i, short_jobs_to_compute.shape[0]))\n",
    "\n",
    "    #real_pow[i], runned_alone[i], quality[i] = calculate_job_consumption(row['job_id_string'])\n",
    "    real_pow_val, runned_alone_val, quality_val, job_n_2_1, job_n_2_2, job_n_3_1 = calculate_job_consumption_short(row['job_id_string'])\n",
    "    short_jobs.at[index,'real_pow'] = real_pow_val\n",
    "    short_jobs.at[index,'runned_alone'] = runned_alone_val\n",
    "    short_jobs.at[index,'real_pow_quality'] = quality_val\n",
    "    \n",
    "    short_jobs.at[index,'n_2_1'] = job_n_2_1\n",
    "    short_jobs.at[index,'n_2_2'] = job_n_2_2\n",
    "    short_jobs.at[index,'n_3_1'] = job_n_3_1\n",
    "    \n",
    "    i = i + 1\n",
    "    if(i % 1000 == 0):\n",
    "        short_jobs.to_csv(outfile)\n",
    "        \n",
    "    #    break\n",
    "\n",
    "#large_jobs['real_pow'] = real_pow\n",
    "#large_jobs['runned_alone'] = runned_alone\n",
    "#large_jobs['pow_quality'] = quality\n",
    "# write\n",
    "short_jobs.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO I probably have to load more than one node measurements file\n",
    "def calculate_job_consumption_short(job_id_string):\n",
    "    \n",
    "    job_real_pow_consumption = 0\n",
    "    job_runned_alone = True\n",
    "    job_pow_quality = 1.0\n",
    "    job_good_nodes = 0 # node where we have at most 1 measurement in the interval\n",
    "    job_measurements_tot = 0\n",
    "    job_measurements_missing_tot = 0\n",
    "    \n",
    "    partial_pow_consumption = 0\n",
    "    \n",
    "    job_to_nodes = merged_jobs_to_nodes_short[merged_jobs_to_nodes_short['job_id_string'] == job_id_string]\n",
    "    job_row = job_to_nodes.iloc[0]\n",
    "    job_to_nodes_unique = job_to_nodes.node_id.unique() # series with the nodes id\n",
    "    ### print some jobs info\n",
    "    print(\"job {} use {} cores in this node\".format(job_id_string, str(job_row['ncpus'])))\n",
    "    #print(\"job {} start at {} and end at {}\".format(job_id_string, str(job_row['run_start_time']), str(job_row['end_time'])))\n",
    "    print(\"job {} requires {} nodes and {} cores\".format(job_id_string, str(job_row['node_req']), str(job_row['cpu_req'])))\n",
    "    print(job_to_nodes_unique)\n",
    "    \n",
    "    job_n_2_1 = 0\n",
    "    job_n_2_2 = 0\n",
    "    job_n_3_1 = 0\n",
    "    \n",
    "    for index, row in job_to_nodes.iterrows():\n",
    "        \n",
    "        node_type = nodes_data.iloc[row['node_id']-1]['cpu_type']\n",
    "        if(node_type == '2_1_ghz'):\n",
    "            job_n_2_1 += 1\n",
    "        elif(node_type == '2_2_ghz'):\n",
    "            job_n_2_2 += 1\n",
    "        elif(node_type == '3_1_ghz'):\n",
    "            job_n_3_1 += 1\n",
    "        \n",
    "        node_idle_core = nodes_data.iloc[row['node_id']-1]['core_idle']\n",
    "        \n",
    "        if(row['node_id'] == 1):\n",
    "            node_measurements = node_measurements_01\n",
    "        elif(row['node_id'] == 2):\n",
    "            node_measurements = node_measurements_02\n",
    "        elif(row['node_id'] == 3):\n",
    "            node_measurements = node_measurements_03\n",
    "        elif(row['node_id'] == 4):\n",
    "            node_measurements = node_measurements_04\n",
    "        elif(row['node_id'] == 5):\n",
    "            node_measurements = node_measurements_05\n",
    "        elif(row['node_id'] == 6):\n",
    "            node_measurements = node_measurements_06\n",
    "        elif(row['node_id'] == 7):\n",
    "            node_measurements = node_measurements_07\n",
    "        elif(row['node_id'] == 8):\n",
    "            node_measurements = node_measurements_08\n",
    "        elif(row['node_id'] == 9):\n",
    "            node_measurements = node_measurements_09\n",
    "        elif(row['node_id'] == 10):\n",
    "            node_measurements = node_measurements_10\n",
    "        elif(row['node_id'] == 11):\n",
    "            node_measurements = node_measurements_11\n",
    "        elif(row['node_id'] == 12):\n",
    "            node_measurements = node_measurements_12\n",
    "        elif(row['node_id'] == 13):\n",
    "            node_measurements = node_measurements_13\n",
    "        elif(row['node_id'] == 14):\n",
    "            node_measurements = node_measurements_14\n",
    "        elif(row['node_id'] == 15):\n",
    "            node_measurements = node_measurements_15\n",
    "        elif(row['node_id'] == 16):\n",
    "            node_measurements = node_measurements_16\n",
    "        elif(row['node_id'] == 17):\n",
    "            node_measurements = node_measurements_17\n",
    "        elif(row['node_id'] == 18):\n",
    "            node_measurements = node_measurements_18\n",
    "        elif(row['node_id'] == 19):\n",
    "            node_measurements = node_measurements_19\n",
    "        elif(row['node_id'] == 20):\n",
    "            node_measurements = node_measurements_20\n",
    "        elif(row['node_id'] == 21):\n",
    "            node_measurements = node_measurements_21\n",
    "        elif(row['node_id'] == 22):\n",
    "            node_measurements = node_measurements_22\n",
    "        elif(row['node_id'] == 23):\n",
    "            node_measurements = node_measurements_23\n",
    "        elif(row['node_id'] == 24):\n",
    "            node_measurements = node_measurements_24\n",
    "        elif(row['node_id'] == 25):\n",
    "            node_measurements = node_measurements_25\n",
    "        elif(row['node_id'] == 26):\n",
    "            node_measurements = node_measurements_26\n",
    "        elif(row['node_id'] == 27):\n",
    "            node_measurements = node_measurements_27\n",
    "        elif(row['node_id'] == 28):\n",
    "            node_measurements = node_measurements_28\n",
    "        elif(row['node_id'] == 29):\n",
    "            node_measurements = node_measurements_29\n",
    "        elif(row['node_id'] == 30):\n",
    "            node_measurements = node_measurements_30\n",
    "        elif(row['node_id'] == 31):\n",
    "            node_measurements = node_measurements_31\n",
    "        elif(row['node_id'] == 32):\n",
    "            node_measurements = node_measurements_32\n",
    "        elif(row['node_id'] == 33):\n",
    "            node_measurements = node_measurements_33\n",
    "        elif(row['node_id'] == 34):\n",
    "            node_measurements = node_measurements_34\n",
    "        elif(row['node_id'] == 35):\n",
    "            node_measurements = node_measurements_35\n",
    "        elif(row['node_id'] == 36):\n",
    "            node_measurements = node_measurements_36\n",
    "        elif(row['node_id'] == 37):\n",
    "            node_measurements = node_measurements_37\n",
    "        elif(row['node_id'] == 38):\n",
    "            node_measurements = node_measurements_38\n",
    "        elif(row['node_id'] == 39):\n",
    "            node_measurements = node_measurements_39\n",
    "        elif(row['node_id'] == 40):\n",
    "            node_measurements = node_measurements_40\n",
    "        elif(row['node_id'] == 41):\n",
    "            node_measurements = node_measurements_41\n",
    "        elif(row['node_id'] == 42):\n",
    "            node_measurements = node_measurements_42\n",
    "        elif(row['node_id'] == 44):\n",
    "            node_measurements = node_measurements_44\n",
    "        elif(row['node_id'] == 45):\n",
    "            node_measurements = node_measurements_45\n",
    "        elif(row['node_id'] == 46):\n",
    "            node_measurements = node_measurements_46\n",
    "        elif(row['node_id'] == 47):\n",
    "            node_measurements = node_measurements_47\n",
    "        elif(row['node_id'] == 48):\n",
    "            node_measurements = node_measurements_48\n",
    "        elif(row['node_id'] == 49):\n",
    "            node_measurements = node_measurements_49\n",
    "        elif(row['node_id'] == 50):\n",
    "            node_measurements = node_measurements_50\n",
    "        elif(row['node_id'] == 51):\n",
    "            node_measurements = node_measurements_51\n",
    "        elif(row['node_id'] == 52):\n",
    "            node_measurements = node_measurements_52\n",
    "        elif(row['node_id'] == 53):\n",
    "            node_measurements = node_measurements_53\n",
    "        elif(row['node_id'] == 54):\n",
    "            node_measurements = node_measurements_54\n",
    "        elif(row['node_id'] == 55):\n",
    "            node_measurements = node_measurements_55\n",
    "        elif(row['node_id'] == 56):\n",
    "            node_measurements = node_measurements_56\n",
    "        elif(row['node_id'] == 57):\n",
    "            node_measurements = node_measurements_57\n",
    "        elif(row['node_id'] == 58):\n",
    "            node_measurements = node_measurements_58\n",
    "        elif(row['node_id'] == 59):\n",
    "            node_measurements = node_measurements_59\n",
    "        elif(row['node_id'] == 60):\n",
    "            node_measurements = node_measurements_60\n",
    "        elif(row['node_id'] == 61):\n",
    "            node_measurements = node_measurements_61\n",
    "        elif(row['node_id'] == 62):\n",
    "            node_measurements = node_measurements_62\n",
    "        elif(row['node_id'] == 63):\n",
    "            node_measurements = node_measurements_63\n",
    "        elif(row['node_id'] == 64):\n",
    "            node_measurements = node_measurements_64\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            ### add a 0 to the number if is less than 10 (\"1\" -> \"01\", ...)\n",
    "            node_str = \"0\" + str(row['node_id']) if row['node_id'] < 10 else str(row['node_id'])\n",
    "            print(\"node {}\".format(node_str))\n",
    "            ### open the measurements node file I/O\n",
    "            infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node_str + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "            node_measurements = pd.read_csv(infile_node)\n",
    "    \n",
    "    \n",
    "        measure = node_measurements[node_measurements['timestamp'] == job_row['run_start_time'][:-3]+':00']\n",
    "        \n",
    "        if(measure.shape[0] != 0):\n",
    "            if(measure['active_cores'].iloc[0] != 0):\n",
    "                job_runned_alone = False\n",
    "            \n",
    "            active_cores = measure['active_cores'].iloc[0] + row['ncpus']\n",
    "            if(active_cores > 16):\n",
    "                active_cores = 16\n",
    "            job_real_pow_consumption = job_real_pow_consumption + (measure['pow_tot_0'].iloc[0] + measure['pow_tot_1'].iloc[0] - (16 - active_cores) * node_idle_core) * row['ncpus'] / active_cores\n",
    "        else:\n",
    "            print(\"error measure\")\n",
    "            job_pow_quality = 0.0\n",
    "    \n",
    "    if(job_real_pow_consumption < 0):\n",
    "        job_real_pow_consumption = 0\n",
    "        job_pow_quality = 0.0\n",
    "    print(job_real_pow_consumption)\n",
    "        \n",
    "    return job_real_pow_consumption, job_runned_alone, job_pow_quality, job_n_2_1, job_n_2_2, job_n_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### calculate the power consumption of a specifified job\n",
    "\n",
    "\n",
    "\n",
    "job_id_string = \"498462.node129\" # non ha misurazioni\n",
    "job_id_string = \"498463.node129\" # non ha misurazioni\n",
    "job_id_string = \"498464.node129\" # non ha misurazioni\n",
    "job_id_string = \"498465.node129\" # [ 35]\n",
    "\n",
    "job_id_string = \"498699.node129\" # [ 37]\n",
    "job_id_string = \"498458.node129\" # non ha misurazioni\n",
    "job_id_string = \"498458.node129\" # non ha misurazioni\n",
    "job_id_string = \"498461.node129\" # single node 16 cores [ 27]\n",
    "job_id_string = \"500752.node129\" # 2 nodi, 24 cores [ 16 20] 20 totalmente assente\n",
    "job_id_string = \"500899.node129\" # [ 15 23 32 28 25 19]\n",
    "\n",
    "job_id_string = \"498467.node129\"\n",
    "\n",
    "job_id_string = \"498460.node129\" # 6 nodi, 96 cores [ 9 28 25 19 26 29]\n",
    "job_id_string = \"498466.node129\" # [ 36 38 40 42 50 56 59 61 62 63 54]\n",
    "job_id_string = \"500752.node129\" # 2 nodi, 24 cores [ 16 20] 20 totalmente assente\n",
    "job_id_string = \"498459.node129\" # single node 16 cores [ 9]\n",
    "job_id_string = \"498699.node129\" # [ 37]\n",
    "job_id_string = \"498458.node129\" # [ ]\n",
    "job_id_string = \"498460.node129\" # 6 nodi, 96 cores [ 9 28 25 19 26 29]\n",
    "job_id_string = \"500899.node129\" # [ 15 23 32 28 25 19]\n",
    "\n",
    "\n",
    "print(\"job_id_string: {}\".format(job_id_string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_job_consumption_2(job_id_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
