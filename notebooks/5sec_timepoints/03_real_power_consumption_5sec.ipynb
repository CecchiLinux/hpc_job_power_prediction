{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import errno    \n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Author: Enrico Ceccolini\n",
    "    TODO write the description\n",
    "'''\n",
    "\n",
    "datadir = \"/datasets/eurora_data/db/\"\n",
    "#datadir = \"/datasets/eurora_data/db/\"\n",
    "#datadir = \"C:/Users/folid/git-repo/hpc_job_power_prediction/datasets/eurora_data/db_local/\"\n",
    "infile_jobs_to_nodes = datadir + \"job_nodes.csv\"\n",
    "\n",
    "\n",
    "infile_nodes = datadir + \"nodes.csv\"\n",
    "\n",
    "suffix = \"_5sec_\"\n",
    "### select an interval from\n",
    "## 1 settings wholeData\n",
    "interval_comment_whole = \"WholeData\"\n",
    "\n",
    "### select an interval from\n",
    "## 2 settings Andrea\n",
    "#interval_comment = \"Andrea\"\n",
    "#start_time = pd.to_datetime('2014-03-31')\n",
    "#end_time = pd.to_datetime('2014-05-01')\n",
    "#infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "## 3 settings Alina\n",
    "interval_comment = \"Alina\"\n",
    "start_time = pd.to_datetime('2014-06-30')\n",
    "end_time = pd.to_datetime('2014-11-01')\n",
    "#infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "## 4 settings BeforeAlina\n",
    "interval_comment = \"Alina\"\n",
    "start_time = pd.to_datetime('2014-03-31')\n",
    "end_time = pd.to_datetime('2014-07-01')\n",
    "infile_jobs = datadir + \"CPUs/jobs_cleaned_whole\"\n",
    "\n",
    "nodes=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the nodes we have three differentIntel Xeon E5 processors stepping: nodes 1-16 & 25-32 havea maximum frequency of 2.1GHz (CPUs-2100), nodes 17-24 have a maximum frequency of 2.2GHz (CPUs-2200), andnodes 33-64 have a maximum frequency of 3.1GHz (CPUs-3100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.read_csv(infile_nodes, index_col=0)\n",
    "# nodes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read job2nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data contains 469095 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = pd.read_csv(infile_jobs_to_nodes, index_col=0)\n",
    "print(\"jobs_to_nodes_whole_data contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of jobs runned on node 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data now contains 469020 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data[jobs_to_nodes_whole_data['node_id'] != 129]\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve the problem of duplicates job_id_string, node_id\n",
    "this is probably caused by the concurrency writing of the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_to_nodes_whole_data now contains 444610 records\n"
     ]
    }
   ],
   "source": [
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data.drop_duplicates(subset=['job_id_string', 'node_id'])\n",
    "print(\"jobs_to_nodes_whole_data now contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read jobs data\n",
    "\n",
    "drop the jobs out of interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_whole_data contains 404792 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "jobs_whole = pd.read_csv(infile_jobs + \".csv\", index_col=0)\n",
    "print(\"jobs_whole_data contains {} records\".format(jobs_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 100003 records\n"
     ]
    }
   ],
   "source": [
    "interval_jobs = jobs_whole[pd.to_datetime(jobs_whole['end_time']) <= end_time]\n",
    "interval_jobs = interval_jobs[pd.to_datetime(interval_jobs['run_start_time']) > start_time]\n",
    "print(\"train set contains {} records\".format(interval_jobs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108074\n"
     ]
    }
   ],
   "source": [
    "merged_jobs_to_nodes = pd.merge(jobs_to_nodes_whole_data, interval_jobs, on='job_id_string')\n",
    "print(merged_jobs_to_nodes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "#merged_jobs_to_node = merged_jobs_to_nodes[merged_jobs_to_nodes['node_id'] == int(node)]\n",
    "#merged_jobs_to_node.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove to continue from a specific job\n",
    "\n",
    "#long_jobs = long_jobs.drop(['real_pow', 'runned_alone', 'real_pow_quality'], axis=1)\n",
    "\n",
    "interval_jobs['real_pow'] = 0.0\n",
    "interval_jobs['ran_alone'] = True\n",
    "interval_jobs['real_pow_quality'] = 0.0\n",
    "\n",
    "interval_jobs['n_2_1'] = 0\n",
    "interval_jobs['n_2_2'] = 0\n",
    "interval_jobs['n_3_1'] = 0\n",
    "\n",
    "interval_jobs['job_tot_timepoints'] = 0\n",
    "interval_jobs['job_timepoints'] = 0\n",
    "interval_jobs['good_nodes'] = 0 # TODO add this on the file? maybe I can drop it at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#debug\n",
    "#jobs_whole = jobs_whole.head(15)\n",
    "#debug_merged_jobs_to_nodes = merged_jobs_to_nodes.head(1000)\n",
    "#merged_jobs_to_nodes[merged_jobs_to_nodes['node_id'] == 55]['end_time']\n",
    "#nodes = ['55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svi\n",
    "#jobs_whole[jobs_whole['job_id_string'] == '1006691.node129'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "outfile = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow_before\"\n",
    "\n",
    "for node in nodes:\n",
    "    merged_jobs_to_node = merged_jobs_to_nodes[merged_jobs_to_nodes['node_id'] == int(node)]\n",
    "    print(\"---------- {} jobs in node {}\".format(merged_jobs_to_node.shape[0], node))\n",
    "    infile_node = datadir + \"CPUs/\" + interval_comment + \"/node\" + node + suffix + interval_comment + \"_active_cores_and_jobs.csv\"\n",
    "    node_measurements = pd.read_csv(infile_node)\n",
    "    node_start_time = node_measurements['timestamp'].iloc[0]\n",
    "    num_minutes = node_measurements.shape[0]\n",
    "    print(\"-------------------node data starts at {}, num minute {}\".format(node_start_time, num_minutes))\n",
    "    \n",
    "    for index_job_to_node, row_job_to_node in merged_jobs_to_node.iterrows():\n",
    "        print()\n",
    "        print(\"{}/{}\".format(i, merged_jobs_to_nodes.shape[0]))\n",
    "        \n",
    "        partial_pow_consumption = 0\n",
    "        job_id_string = row_job_to_node['job_id_string']\n",
    "        \n",
    "        # recupero indice del job in interval_jobs\n",
    "        job = interval_jobs[interval_jobs['job_id_string'] == job_id_string]\n",
    "        job_index = interval_jobs[interval_jobs['job_id_string'] == job_id_string].index\n",
    "        if(job_index != None):\n",
    "            job_real_consumption = job['real_pow'].iloc[0]\n",
    "            job_n_2_1 = job['n_2_1'].iloc[0]\n",
    "            job_n_2_2 = job['n_2_2'].iloc[0]\n",
    "            job_n_3_1 = job['n_3_1'].iloc[0]\n",
    "\n",
    "            job_tot_timepoints = job['job_tot_timepoints'].iloc[0]\n",
    "            job_timepoints = job['job_timepoints'].iloc[0]\n",
    "            job_good_nodes = job['good_nodes'].iloc[0]\n",
    "            job_ran_alone = job['ran_alone'].iloc[0]\n",
    "\n",
    "            node_type = nodes_data.iloc[row_job_to_node['node_id']-1]['cpu_type']\n",
    "            if(node_type == '2_1_ghz'):\n",
    "                job_n_2_1 += 1\n",
    "            elif(node_type == '2_2_ghz'):\n",
    "                job_n_2_2 += 1\n",
    "            elif(node_type == '3_1_ghz'):\n",
    "                job_n_3_1 += 1\n",
    "\n",
    "            node_idle_core = nodes_data.iloc[row_job_to_node['node_id']-1]['core_idle']\n",
    "            \n",
    "            job_start_time = pd.to_datetime(row_job_to_node['run_start_time'])\n",
    "            job_end_time = pd.to_datetime(row_job_to_node['end_time'])\n",
    "            \n",
    "            if(job_end_time - job_start_time >= np.timedelta64(5, 's')):\n",
    "                before_minutes = int((job_start_time - pd.to_datetime(node_start_time)) / np.timedelta64(5, 's'))\n",
    "                running_minutes = int((job_end_time - job_start_time) / np.timedelta64(5, 's'))\n",
    "                after_minutes = num_minutes - running_minutes - before_minutes\n",
    "                \n",
    "                before_serie = pd.Series(False, index=np.arange(before_minutes))\n",
    "                running_serie = pd.Series(True, index=np.arange(running_minutes))\n",
    "                after_serie = pd.Series(False, index=np.arange(after_minutes))\n",
    "                concat_series = pd.concat([before_serie, running_serie, after_serie], ignore_index=True)\n",
    "                \n",
    "                interval_measurements = node_measurements[concat_series]\n",
    "                \n",
    "                ### take only the entries of the interval where the job was running\n",
    "                #interval_measurements = node_measurements.loc[(pd.to_datetime(node_measurements['timestamp']) >= pd.to_datetime(row_job_to_node['run_start_time'])) & (pd.to_datetime(node_measurements['timestamp']) <= pd.to_datetime((row_job_to_node['end_time'])) - np.timedelta64(5, 's'))]\n",
    "                ### drop missing measurements (calculate the percentage)\n",
    "                n_node_measurements = interval_measurements.shape[0]\n",
    "                job_tot_timepoints += n_node_measurements\n",
    "\n",
    "                interval_measurements = interval_measurements.dropna()\n",
    "                print(\"{}/{} intervals missing\".format(n_node_measurements-interval_measurements.shape[0], n_node_measurements))\n",
    "                job_timepoints += n_node_measurements-interval_measurements.shape[0]\n",
    "\n",
    "                ### drop where active cores is greater than 16 - this problem occours only once on node 30\n",
    "                interval_measurements = interval_measurements[interval_measurements['active_cores'] <= 16]\n",
    "                ### drop row with 0 active_cores (or less than the current job used cores)\n",
    "                interval_measurements = interval_measurements[interval_measurements['active_cores'] >= row_job_to_node['ncpus']]  \n",
    "\n",
    "                if(interval_measurements.shape[0] != 0):\n",
    "                    job_good_nodes += 1 \n",
    "\n",
    "\n",
    "                ### group the intervals wehere the partial_pow_cons can be obtaied with the same instance of the formula\n",
    "                ### take the mean for the pow columns\n",
    "                # interval_grouped = interval_measurements.reset_index().groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"]).mean()\n",
    "                interval_grouped = interval_measurements.groupby([\"active_cores\", \"active_jobs\", \"active_gpus\", \"active_mics\"])\n",
    "                counts = interval_grouped.size().to_frame(name='counts')\n",
    "                interval_grouped = (counts\n",
    "                 .join(interval_grouped.agg({'pow_tot':'mean'}).rename(columns={'pow_tot': 'pow_tot_mean'}))\n",
    "                 .reset_index()\n",
    "                )\n",
    "                interval_grouped.sort_values('active_jobs')\n",
    "\n",
    "                if(interval_grouped.shape[0]>1):\n",
    "                    job_ran_alone = False\n",
    "\n",
    "\n",
    "                if(interval_grouped.shape[0] != 0):\n",
    "                    #interval_grouped['pow_tot'] = (interval_grouped['pow_tot_0_mean'] + interval_grouped['pow_tot_1_mean']) * row['ncpus'] / interval_grouped['active_cores'] \n",
    "                    interval_grouped['pow_tot'] = (interval_grouped['pow_tot_mean'] - (16 - interval_grouped['active_cores'])*node_idle_core) * row_job_to_node['ncpus'] / interval_grouped['active_cores'] \n",
    "\n",
    "                    # not_alone_counts = interval_grouped['counts'].loc[interval_grouped['active_jobs'] != 1].sum()\n",
    "                    # interval_grouped.loc[interval_grouped['active_jobs'] == 1, ['counts']] += not_alone_counts\n",
    "                    print(interval_grouped)\n",
    "                    partial_pow_consumption = np.average(interval_grouped['pow_tot'], weights=interval_grouped['counts'])\n",
    "                    # partial_pow_consumption = partial_pow_consumption / interval_grouped.shape[0]\n",
    "                    # print(interval_grouped)\n",
    "\n",
    "                print(\"partial measurement: {}\".format(partial_pow_consumption))\n",
    "                job_real_consumption += partial_pow_consumption\n",
    "                partial_pow_consumption = 0\n",
    "\n",
    "        \n",
    "            else: # jobs shorter than 5 sec\n",
    "                   # keep the only timepoint\n",
    "                measure = node_measurements[node_measurements['timestamp'] == row_job_to_node['run_start_time'][:-3]+':00']\n",
    "                job_tot_timepoints += 1\n",
    "                if(measure.shape[0] != 0):\n",
    "                    job_timepoints += 1\n",
    "                    job_good_nodes += 1\n",
    "                    if(measure['active_cores'].iloc[0] != 0):\n",
    "                        job_ran_alone = False\n",
    "\n",
    "                    active_cores = measure['active_cores'].iloc[0] + row_job_to_node['ncpus']\n",
    "                    if(active_cores > 16):\n",
    "                        active_cores = 16\n",
    "                    job_real_consumption += (measure['pow_tot'].iloc[0] - (16 - active_cores) * node_idle_core) * row_job_to_node['ncpus'] / active_cores\n",
    "                else:\n",
    "                    print(\"error measure\")\n",
    "                    job_pow_quality = 0.0\n",
    "               \n",
    "            \n",
    "            if(job_real_consumption < 0):\n",
    "                job_real_consumption = 0.0\n",
    "            print(\"job_real_consumption: {}\".format(job_real_consumption))\n",
    "            \n",
    "            interval_jobs.at[job_index, 'real_pow'] = job_real_consumption\n",
    "            interval_jobs.at[job_index, 'n_2_1'] = job_n_2_1\n",
    "            interval_jobs.at[job_index, 'n_2_2'] = job_n_2_2\n",
    "            interval_jobs.at[job_index, 'n_3_1'] = job_n_3_1\n",
    "            interval_jobs.at[job_index, 'job_tot_timepoints'] = job_tot_timepoints\n",
    "            interval_jobs.at[job_index, 'job_timepoints'] = job_timepoints\n",
    "            interval_jobs.at[job_index, 'good_nodes'] = job_good_nodes\n",
    "            interval_jobs.at[job_index, 'ran_alone'] = job_ran_alone\n",
    "    \n",
    "        \n",
    "        i = i + 1\n",
    "    #if(i % 10000 == 0):\n",
    "    interval_jobs.to_csv(outfile + \"_\" + node + \"_\" + str(i) + \".csv\")\n",
    "        \n",
    "interval_jobs.to_csv(outfile + \"_\" + node + \"_\" + str(i) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
