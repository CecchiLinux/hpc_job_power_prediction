{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import errno    \n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "'''\n",
    "Author: Enrico Ceccolini\n",
    "    TODO write the description\n",
    "'''\n",
    "\n",
    "#datadir = \"/datasets/eurora_data/db1/\"\n",
    "datadir = \"C:/Users/folid/git-repo/hpc_job_power_prediction/datasets/eurora_data/db_local/\"\n",
    "infile_jobs_to_nodes = datadir + \"job_nodes.csv\"\n",
    "\n",
    "infile_nodes = datadir + \"nodes.csv\"\n",
    "\n",
    "suffix = \"_5sec_\"\n",
    "### select an interval from\n",
    "## 1 settings wholeData\n",
    "interval_comment_whole = \"WholeData\"\n",
    "\n",
    "### select an interval from\n",
    "## 2 settings Andrea\n",
    "#interval_comment = \"Andrea\"\n",
    "#start_time = pd.to_datetime('2014-03-31')\n",
    "#end_time = pd.to_datetime('2014-05-01')\n",
    "#infile_jobs = datadir + \"CPUs/\" + interval_comment + \"/jobs_cleaned\"\n",
    "\n",
    "## 3 settings Alina\n",
    "interval_comment = \"Alina\"\n",
    "start_time = pd.to_datetime('2014-06-30')\n",
    "end_time = pd.to_datetime('2014-11-01')\n",
    "\n",
    "#infile_large_jobs = datadir + \"CPUs/WholeData/large_jobs_real_pow_final.csv\"\n",
    "#outfile_large_jobs = datadir + \"CPUs/WholeData/large_jobs_real_pow_final.csv\"\n",
    "\n",
    "infile_jobs = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow\"\n",
    "outfile_jobs = datadir + 'CPUs/' + interval_comment + \"/\" + interval_comment + \"_long_jobs_real_pow_res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_to_nodes_whole_data = pd.read_csv(infile_jobs_to_nodes, index_col=0)\n",
    "print(\"jobs_to_nodes_whole_data contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))\n",
    "\n",
    "### clean the data\n",
    "# remove jobs runned on the inexistent node 129\n",
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data[jobs_to_nodes_whole_data['node_id'] != 129] \n",
    "# remove jobs with the same id that runned on the same node\n",
    "jobs_to_nodes_whole_data = jobs_to_nodes_whole_data.drop_duplicates(subset=['job_id_string', 'node_id'])\n",
    "print(\"after the clean, jobs_to_nodes_whole_data contains {} records\".format(jobs_to_nodes_whole_data.shape[0]))\n",
    "\n",
    "jobs = pd.read_csv(infile_jobs + \".csv\", index_col=0)\n",
    "merged_jobs_to_nodes = pd.merge(jobs_to_nodes_whole_data, jobs, on='job_id_string')\n",
    "print(merged_jobs_to_nodes.shape[0])\n",
    "merged_jobs_to_nodes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "count = 0\n",
    "for index, row in jobs.iterrows():\n",
    "    \n",
    "    i += 1\n",
    "    if (row['node_req'] == 0):\n",
    "        count += 1\n",
    "        job_to_nodes = merged_jobs_to_nodes[merged_jobs_to_nodes['job_id_string'] == row['job_id_string']]\n",
    "        nnodes = job_to_nodes.shape[0]\n",
    "        ncpus = job_to_nodes['ncpus'].sum()\n",
    "        ngpus = job_to_nodes['ngpus'].sum()\n",
    "        nmics = job_to_nodes['nmics'].sum()\n",
    "        mem_requested = job_to_nodes['mem_requested'].sum()\n",
    "\n",
    "        #print(ncpus)\n",
    "        jobs.at[index, 'node_req'] = nnodes\n",
    "        jobs.at[index, 'cpu_req'] = ncpus\n",
    "        jobs.at[index, 'gpu_req'] = ngpus\n",
    "        jobs.at[index, 'mic_req'] = nmics\n",
    "        jobs.at[index, 'mem_req'] = mem_requested\n",
    "    if (i%10000 == 0):\n",
    "        jobs.to_csv(outfile_jobs + \".csv\")\n",
    "    if (count % 1000 == 0):\n",
    "        print(count)\n",
    "        print(\"{}/{}\".format(i, jobs.shape[0]))\n",
    "        \n",
    "print(count)\n",
    "jobs.to_csv(outfile_jobs + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[jobs['node_req'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.to_csv(outfile_jobs + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
